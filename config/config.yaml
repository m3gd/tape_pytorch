defaults:
  - _self_
  - model: tape
  - optimizer: lion
  - scheduler: cosine
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog

# Training settings
train:
  dataset_name: cifar10  # Set this to use a dataset from the HF hub
  dataset_config_name: null
  train_data_dir: null  # Set this to use local data
  cache_dir: null
  seed: 42
  output_dir: outputs/${model.name}-${train.dataset_name}-${train.resolution}-${now:%Y%m%d_%H%M%S}
  resolution: 32
  center_crop: false
  random_flip: true
  train_batch_size: 64
  eval_batch_size: 64
  num_workers: 8
  num_epochs: 1500
  gradient_accumulation_steps: 4
  mixed_precision: "no"  # Choose between "no", "fp16", "bf16"
  enable_xformers_memory_efficient_attention: false
  
# Diffusion settings
diffusion:
  prediction_type: "epsilon"  # Choose between "epsilon" or "sample"
  num_train_timesteps: 1000
  num_inference_steps: 1000
  beta_schedule: "linear"
  
# Checkpointing and logging
checkpoint:
  save_images_epochs: 20
  save_model_epochs: 20
  checkpointing_steps: 500
  checkpoints_total_limit: 3
  resume_from_checkpoint: null
  
# Logging
logging:
  logger: "wandb"  # Choose between "tensorboard" or "wandb"
  logging_dir: "logs"
  wandb:
    project: "tape-diffusion"
    entity: null  # Your wandb username or team name
    
# Hub upload settings
hub:
  push_to_hub: false
  hub_model_id: null
  hub_token: null
  hub_private_repo: false

# Hydra settings
hydra:
  # output_subdir: config # hydra config would be stored in .hydra
  run:
    dir: ${train.output_dir}
  sweep:
    dir: multirun
    subdir: ${hydra.job.num}
  job:
    chdir: true
